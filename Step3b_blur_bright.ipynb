{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries"
      ],
      "metadata": {
        "id": "uSKyMbrAV17l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhbiuW2BU9Ec"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchmetrics thop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2od11trTFic"
      },
      "outputs": [],
      "source": [
        "! pip install torchprofile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP_mEN9aTPSC"
      },
      "outputs": [],
      "source": [
        "!pip install -U fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdsXf207N8gV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "#from torchmetrics import JaccardIndex\n",
        "#from thop import profile, clever_format\n",
        "import time\n",
        "\n",
        "#from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "#import torchprofile\n",
        "\n",
        "#from models.deeplabv2.deeplabv2 import get_deeplab_v2\n",
        "\n",
        "\n",
        "from torchmetrics import JaccardIndex\n",
        "from thop import profile, clever_format\n",
        "import time\n",
        "\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import torchprofile\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNWddRGrt1kp"
      },
      "source": [
        "**Cityscapes Dataset Download**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4k4DE0eEjwr"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import pathlib\n",
        "drive.mount('/content/Drive')\n",
        "get_ipython().system('/content/Drive/MyDrive/Cityscapes/Cityscapes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqu-NDJHFkGk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuEJmYHCeerC"
      },
      "source": [
        "**GTA5 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjU52v_9P6Wj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pathlib\n",
        "drive.mount('/content/Drive')\n",
        "get_ipython().system('/content/Drive/MyDrive/GTA5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_09FKOE7XLEG"
      },
      "source": [
        "**Model Clone**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPllYnf-NPeU"
      },
      "outputs": [],
      "source": [
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kkLfxwKNZ5J"
      },
      "outputs": [],
      "source": [
        "# Navigate to the project directory\n",
        "%cd MLDL2024_project1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi5RIWMWOJba"
      },
      "outputs": [],
      "source": [
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAC9VIa40hM9"
      },
      "outputs": [],
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnnzlL6eVNsV"
      },
      "source": [
        "**Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "032jMg_kVQWJ"
      },
      "outputs": [],
      "source": [
        "# Define training parameters\n",
        "epochs = 50\n",
        "learning_rate = 0.0001\n",
        "batch_size = 4\n",
        "train_resolution = (1280,720) #(1024, 512)\n",
        "test_resolution = (1024, 512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3HtXvfzRp1R"
      },
      "outputs": [],
      "source": [
        "class CityscapesDataset(Dataset):\n",
        "  def __init__(self, root_dir, im_transform ):\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    root_dir (string): Directory with all the images.\n",
        "    transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "    self.root_dir = root_dir\n",
        "    self.im_transform = im_transform\n",
        "\n",
        "    self.images = []\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "      for file in files:\n",
        "          self.images.append(os.path.join(subdir, file))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = self.images[idx]\n",
        "    image = Image.open(img_name).convert('RGB')\n",
        "    label_name = img_name.replace('images', 'gtFine').replace('_leftImg8bit','_gtFine_labelTrainIds')  #labelTrainIds\n",
        "\n",
        "\n",
        "    label = Image.open(label_name).convert('L')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Resize label using nearest-neighbor interpolation\n",
        "    label = TF.resize(label, (512,1024 ), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    label = np.array(label)  # Convert to numpy array\n",
        "    label = torch.from_numpy(label).long()  # Convert to LongTensor\n",
        "\n",
        "\n",
        "    if self.im_transform:\n",
        "\n",
        "      image = self.im_transform(image)\n",
        "\n",
        "\n",
        "    return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuDK4JddkZnR"
      },
      "outputs": [],
      "source": [
        "class GTA5Dataset(Dataset):\n",
        "  def __init__(self, root_dir, im_transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    root_dir (string): Directory with all the images and labels.\n",
        "    im_transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "    self.root_dir = root_dir\n",
        "    self.im_transform = im_transform\n",
        "    self.transform=im_transform\n",
        "    self.images_dir = os.path.join(root_dir, 'images')\n",
        "    self.labels_dir = os.path.join(root_dir, 'labels')\n",
        "    self.images = [os.path.join(self.images_dir, file) for file in os.listdir(self.images_dir) if file.endswith('.png')]\n",
        "    self.id_to_trainid = {7: 0, 8: 1, 11: 2, 12: 3, 13: 4, 17: 5,\n",
        "                              19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12,\n",
        "                              26: 13, 27: 14, 28: 15, 31: 16, 32: 17, 33: 18}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = self.images[idx]\n",
        "    image = Image.open(img_name).convert('RGB')\n",
        "    label_name = img_name.replace('images', 'labels')\n",
        "    label = Image.open(label_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     # resize\n",
        "\n",
        "    image = image.resize((1280,720), Image.BICUBIC)\n",
        "    label = label.resize((1280,720), Image.NEAREST)\n",
        "\n",
        "    image = np.array(image)\n",
        "    label = np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     # re-assign labels to match the format of Cityscapes\n",
        "    label_copy = 255 * np.ones(label.shape)\n",
        "    for k, v in self.id_to_trainid.items():\n",
        "        label_copy[label == k] = v\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    label_copy = torch.from_numpy(label_copy).long()  # Convert to LongTensor\n",
        "\n",
        "    if self.im_transform:\n",
        "      image = self.im_transform(image)\n",
        "\n",
        "\n",
        "\n",
        "    return image, label_copy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RxFO23631_3"
      },
      "source": [
        "**GTA5 Augmented Train Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0RCkMU_lAhz"
      },
      "outputs": [],
      "source": [
        "\n",
        "###-----   Augmentation Transforms:\n",
        "\n",
        "# Create the dataset with augmentations\n",
        "im_transform = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "#transforms.Resize(train_resolution),\n",
        "#transforms.RandomHorizontalFlip(p=0.5),  # Apply horizontal flip with 50% probability\n",
        "transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.5),  # Apply Gaussian Blur with 50% probability\n",
        "transforms.RandomApply([transforms.ColorJitter(brightness=0.5)], p=0.5),  # Apply brightness adjustment with 50% probability\n",
        "\n",
        "                                    ])\n",
        "\n",
        "\n",
        "\n",
        "gta5_dataset = GTA5Dataset(root_dir='/content/Drive/MyDrive/GTA5', im_transform=im_transform)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xskaQe0nV0K"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(gta5_dataset, batch_size=batch_size, shuffle=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UszT9qhVJl44"
      },
      "source": [
        "**Citescapes Train Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0Yr3sirzs00"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the BiSeNet model\n",
        "model = BiSeNet(num_classes=19,context_path='resnet18')  #BiSeNet\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Define the CrossEntropyLoss with ignore_index set to 255\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fNiWtGSzs8P"
      },
      "outputs": [],
      "source": [
        "# Define the metric for mIoU\n",
        "miou_metric = JaccardIndex(num_classes=19, task='multiclass' , ignore_index=255).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8p-LLxDzMNb"
      },
      "outputs": [],
      "source": [
        "# Function to compute latency\n",
        "def measure_latency(model, input_tensor, repetitions=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        for _ in range(repetitions):\n",
        "            _ = model(input_tensor)\n",
        "        end = time.time()\n",
        "    latency = (end - start) / repetitions\n",
        "    return latency\n",
        "\n",
        "\n",
        "\n",
        "# Measure FLOPs and number of parameters\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 1024, 512).to(device)\n",
        "\n",
        "height = 1024\n",
        "width = 512\n",
        "image =torch.zeros((1,3, height, width)).to(device)   # torch.randn(1,3, 1024, 512).to(device)#\n",
        "\n",
        "model.eval()\n",
        "flops = FlopCountAnalysis(model, image)\n",
        "print(flop_count_table(flops))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljOeeMI_Qlp6"
      },
      "outputs": [],
      "source": [
        "# Measure FLOPs and parameters using torchprofile\n",
        "dummy_input = torch.randn(1, 3, 1024,512).to(device)\n",
        "model.eval()\n",
        "flops = torchprofile.profile_macs(model, args=(dummy_input,))\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f' flops={flops}\\n params={params} ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFgrmuBxbZL_"
      },
      "source": [
        "**IoU Calculation Function for Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpeSv9KSbXv6"
      },
      "outputs": [],
      "source": [
        "def fast_hist(a, b, n):\n",
        "    '''\n",
        "    a and b are label and prediction respectively\n",
        "    n is the number of classes\n",
        "    '''\n",
        "    #k = (a >= 0) & (a < n)\n",
        "    k = (b >= 0) & (b < n)\n",
        "    return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
        "\n",
        "\n",
        "def per_class_iou(hist):\n",
        "    epsilon = 1e-5\n",
        "    return (np.diag(hist)) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Iwy_KA5TNwE"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kxTz4ev204g"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/Drive')\n",
        "get_ipython().system('/content/Drive/MyDrive/Checkpoints/3b/3b_blur_bright_0.0001_4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZKhvu742y5R"
      },
      "outputs": [],
      "source": [
        "# Function to save the model\n",
        "def save_checkpoint(epoch, model, optimizer, save_dir='/content/Drive/MyDrive/Checkpoints/3b/3b_blur_bright_0.0001_4'):\n",
        "    # Ensure the save directory exists\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Define the model filename with the epoch number\n",
        "    checkpoint_filename = f'checkpoint_epoch_{epoch}.pth'\n",
        "    checkpoint_path = os.path.join(save_dir, checkpoint_filename)\n",
        "\n",
        "    # Save the model and optimizer state dictionaries\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f'Model and optimizer saved to {checkpoint_path}')\n",
        "\n",
        "# Function to load the model\n",
        "def load_checkpoint(epoch, model, optimizer, save_dir='/content/Drive/MyDrive/Checkpoints/3b/3b_blur_bright_0.0001_4'):\n",
        "    checkpoint_filename = f'checkpoint_epoch_{epoch}.pth'\n",
        "    checkpoint_path = os.path.join(save_dir, checkpoint_filename)\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.isfile(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"The specified file was not found: {checkpoint_path}\")\n",
        "\n",
        "    # Load the model and optimizer state dictionaries\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "    print(f'Model and optimizer loaded from {checkpoint_path}, resuming at epoch {start_epoch}')\n",
        "    return model, optimizer, start_epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL_bZe4S2ynQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1T9gxM10O1P"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "# Resume training from the last checkpoint if available\n",
        "resume_training =True   # Set this to True if you want to resume training\n",
        "\n",
        "epochs = 50  # Set this to the total number of epochs you want to train\n",
        "\n",
        "if resume_training:\n",
        "    epoch_to_resume =19 #9,19,29,39,49  # Set this to the epoch from which you want to resume\n",
        "    try:\n",
        "        model, optimizer, start_epoch = load_checkpoint(epoch_to_resume, model, optimizer)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        start_epoch = 0\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "for epoch in range(start_epoch,epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    miou_metric.reset()\n",
        "    counter=1\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        #labels = labels.squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs[0], labels.long())\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(counter)\n",
        "        counter+=1\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        miou_metric.update(outputs[0].argmax(dim=1), labels)\n",
        "\n",
        "     # Save the model every 10 epochs\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        save_checkpoint(epoch, model, optimizer)\n",
        "\n",
        "    miou = miou_metric.compute().item()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}, Train mIoU: {miou}')\n",
        "\n",
        "# Measure latency after training\n",
        "latency = measure_latency(model, dummy_input)\n",
        "print(f\"FLOPs: {flops}, Params: {params}, Latency: {latency:.6f} seconds\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTh46WFVkFLq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpQ67CRAKM-t"
      },
      "source": [
        "**Test Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8F7HxrMSK3X"
      },
      "outputs": [],
      "source": [
        "# Define a transform if you need to preprocess the images\n",
        "transformed_dataset = CityscapesDataset(root_dir='/content/Drive/MyDrive/Cityscapes/Cityscapes/images/val',\n",
        "im_transform=transforms.Compose([\n",
        "\n",
        "transforms.ToTensor(),\n",
        "transforms.Resize((512,1024)),\n",
        "\n",
        "]), )\n",
        "\n",
        "\n",
        "test_loader = DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ3uZ37_S_5C"
      },
      "source": [
        "**Inference and mIoU calculation on test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZd3_wXY4DbS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "model.eval()\n",
        "miou_metric.reset()\n",
        "\n",
        "all_classes_iou=np.zeros(19)\n",
        "test_counter=0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #calcualting IoU for each class and for each (image,label) pair separately\n",
        "        current_batch_size = labels.size(0)\n",
        "        for i in range(current_batch_size):\n",
        "\n",
        "            mask=labels[i,:,:].cpu().numpy().flatten() != 255\n",
        "            #hist=fast_hist(outputs[i,:,:,:].argmax(dim=0).cpu().numpy().flatten()[mask] , labels[i,:,:].cpu().numpy().flatten()[mask], 19)\n",
        "\n",
        "            hist=fast_hist(outputs[i,:,:,:].argmax(dim=0).cpu().numpy().flatten() , labels[i,:,:].cpu().numpy().flatten(), 19)\n",
        "\n",
        "            all_classes_iou=all_classes_iou+per_class_iou(hist)\n",
        "            test_counter = test_counter +1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwqEwifUcjgf"
      },
      "outputs": [],
      "source": [
        "test_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3roYr0uWqMQ"
      },
      "outputs": [],
      "source": [
        "all_classes_mIOU=(all_classes_iou/test_counter).round(3)  #calculating mean intersection over union for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cZkG6AMWy5f"
      },
      "outputs": [],
      "source": [
        "all_classes_mIOU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhoYFSI4XY3p"
      },
      "outputs": [],
      "source": [
        "mIoU=all_classes_mIOU.mean()  #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKud9H4MXcjx"
      },
      "outputs": [],
      "source": [
        "mIoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQDH0xjZdCoZ"
      },
      "source": [
        "**GTA5 Labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MktUOoKOdGMq"
      },
      "outputs": [],
      "source": [
        "from abc import ABCMeta\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "class BaseGTALabels(metaclass=ABCMeta):\n",
        "    pass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GTA5Label:\n",
        "    ID: int\n",
        "    color: Tuple[int, int, int]\n",
        "\n",
        "\n",
        "class GTA5Labels_TaskCV2017(BaseGTALabels):\n",
        "    road = GTA5Label(ID=0, color=(128, 64, 128))\n",
        "    sidewalk = GTA5Label(ID=1, color=(244, 35, 232))\n",
        "    building = GTA5Label(ID=2, color=(70, 70, 70))\n",
        "    wall = GTA5Label(ID=3, color=(102, 102, 156))\n",
        "    fence = GTA5Label(ID=4, color=(190, 153, 153))\n",
        "    pole = GTA5Label(ID=5, color=(153, 153, 153))\n",
        "    light = GTA5Label(ID=6, color=(250, 170, 30))\n",
        "    sign = GTA5Label(ID=7, color=(220, 220, 0))\n",
        "    vegetation = GTA5Label(ID=8, color=(107, 142, 35))\n",
        "    terrain = GTA5Label(ID=9, color=(152, 251, 152))\n",
        "    sky = GTA5Label(ID=10, color=(70, 130, 180))\n",
        "    person = GTA5Label(ID=11, color=(220, 20, 60))\n",
        "    rider = GTA5Label(ID=12, color=(255, 0, 0))\n",
        "    car = GTA5Label(ID=13, color=(0, 0, 142))\n",
        "    truck = GTA5Label(ID=14, color=(0, 0, 70))\n",
        "    bus = GTA5Label(ID=15, color=(0, 60, 100))\n",
        "    train = GTA5Label(ID=16, color=(0, 80, 100))\n",
        "    motocycle = GTA5Label(ID=17, color=(0, 0, 230))\n",
        "    bicycle = GTA5Label(ID=18, color=(119, 11, 32))\n",
        "\n",
        "    list_ = [\n",
        "        road,\n",
        "        sidewalk,\n",
        "        building,\n",
        "        wall,\n",
        "        fence,\n",
        "        pole,\n",
        "        light,\n",
        "        sign,\n",
        "        vegetation,\n",
        "        terrain,\n",
        "        sky,\n",
        "        person,\n",
        "        rider,\n",
        "        car,\n",
        "        truck,\n",
        "        bus,\n",
        "        train,\n",
        "        motocycle,\n",
        "        bicycle,\n",
        "    ]\n",
        "\n",
        "    @property\n",
        "    def support_id_list(self):\n",
        "        ret = [label.ID for label in self.list_]\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0BRk3RlZNiY"
      },
      "outputs": [],
      "source": [
        "class_labels = []\n",
        "for label_name, label in GTA5Labels_TaskCV2017.__dict__.items():\n",
        "  if isinstance(label, GTA5Label):\n",
        "    class_labels.append(label_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcvRxtpLZU1s"
      },
      "outputs": [],
      "source": [
        "class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLgml_o1al2p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR6dfnTWZNq6"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame\n",
        "all_classes_mIOU_DF = pd.DataFrame({\n",
        "'class_Label': class_labels,\n",
        "'class_IoU': all_classes_mIOU\n",
        "})\n",
        "\n",
        "all_classes_mIOU_DF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "280Lf-oktoep",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "52585148-e178-402a-faae-0720b6d1e069"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title class_IoU\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "all_classes_mIOU_DF['class_IoU'].plot(kind='hist', bins=20, title='class_IoU')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3BFrswajh3L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agdDp61Fi41m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCwYG2xri45j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}