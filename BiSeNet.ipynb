{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f33gyqE2K2JJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary libraries\n"
      ],
      "metadata": {
        "id": "jSbu5LDvAfHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torch torchvision torchmetrics thop\n"
      ],
      "metadata": {
        "id": "PhbiuW2BU9Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchprofile"
      ],
      "metadata": {
        "id": "W2od11trTFic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U fvcore"
      ],
      "metadata": {
        "id": "kP_mEN9aTPSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NuilIoSNyu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import os\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "#from torchmetrics import JaccardIndex\n",
        "#from thop import profile, clever_format\n",
        "import time\n",
        "\n",
        "#from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "#import torchprofile\n",
        "\n",
        "\n",
        "\n",
        "from torchmetrics import JaccardIndex\n",
        "from thop import profile, clever_format\n",
        "import time\n",
        "\n",
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "import torchprofile\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KdsXf207N8gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBTPWcocSrR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#define CityscapesDataset"
      ],
      "metadata": {
        "id": "ugSDQ-naAsVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CityscapesDataset(Dataset):\n",
        "  def __init__(self, root_dir, im_transform ):\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    root_dir (string): Directory with all the images.\n",
        "    transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "    self.root_dir = root_dir\n",
        "    self.im_transform = im_transform\n",
        "    #self.lab_transform = lab_transform\n",
        "    self.images = []\n",
        "    for subdir, dirs, files in os.walk(root_dir):\n",
        "      for file in files:\n",
        "        if file.endswith('_gtFine_color.png'):\n",
        "          self.images.append(os.path.join(subdir, file))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_name = self.images[idx]\n",
        "    image = Image.open(img_name).convert('RGB')\n",
        "    label_name = img_name.replace('_gtFine_color.png', '_gtFine_labelTrainIds.png')  #labelTrainIds\n",
        "    label = Image.open(label_name)\n",
        "\n",
        "\n",
        "    # Resize label using nearest-neighbor interpolation\n",
        "    label = TF.resize(label, (1024, 512), interpolation=transforms.InterpolationMode.NEAREST)\n",
        "    label = np.array(label)  # Convert to numpy array\n",
        "    label = torch.from_numpy(label).long()  # Convert to LongTensor\n",
        "\n",
        "\n",
        "    if self.im_transform:\n",
        "\n",
        "      image = self.im_transform(image)\n",
        "\n",
        "    # if self.lab_transform:\n",
        "    #   label = self.lab_transform(label)\n",
        "\n",
        "    return image, label\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3HtXvfzRp1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYwHw7jwvTI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cityscapes Dataset load"
      ],
      "metadata": {
        "id": "zNWddRGrt1kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/Drive')\n",
        "get_ipython().system('/content/Drive/MyDrive/Cityscapes/Cityspaces')"
      ],
      "metadata": {
        "id": "E4k4DE0eEjwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system('/content/Drive/MyDrive/Cityscapes/Cityspaces')"
      ],
      "metadata": {
        "id": "wqu-NDJHFkGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Clone**"
      ],
      "metadata": {
        "id": "_09FKOE7XLEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/Gabrysse/MLDL2024_project1.git\n",
        "%cd MLDL2024_project1"
      ],
      "metadata": {
        "id": "HPllYnf-NPeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54MyjXbIZrqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdvvn2sDZr2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the project directory\n",
        "%cd MLDL2024_project1"
      ],
      "metadata": {
        "id": "7kkLfxwKNZ5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from models.bisenet.build_bisenet import BiSeNet"
      ],
      "metadata": {
        "id": "Vi5RIWMWOJba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "maAKFDMmv7V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "PAC9VIa40hM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NlAkhe7VNNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters**"
      ],
      "metadata": {
        "id": "mnnzlL6eVNsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training parameters\n",
        "epochs = 25\n",
        "learning_rate =0.001\n",
        "batch_size = 4\n",
        "train_resolution = (1024, 512)\n",
        "test_resolution = (1024, 512)\n"
      ],
      "metadata": {
        "id": "032jMg_kVQWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjeViRqLVQ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Loader**"
      ],
      "metadata": {
        "id": "UszT9qhVJl44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform if you need to preprocess the images\n",
        "transformed_dataset = CityscapesDataset(root_dir='/content/Drive/MyDrive/Cityscapes/Cityspaces/gtFine/train',\n",
        "im_transform=transforms.Compose([\n",
        "\n",
        "transforms.ToTensor(),\n",
        "transforms.Resize(train_resolution),\n",
        "\n",
        "]), )\n",
        "\n",
        "\n",
        "train_loader = DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "print(len(train_loader))\n"
      ],
      "metadata": {
        "id": "gjWP2n62UBn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the BiSeNet model\n",
        "model = BiSeNet(num_classes=19,context_path='resnet101')   #BiSeNet\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Define the CrossEntropyLoss with ignore_index set to 255\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "F0Yr3sirzs00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the metric for mIoU\n",
        "miou_metric = JaccardIndex(num_classes=19, task='multiclass' , ignore_index=255).to(device)\n"
      ],
      "metadata": {
        "id": "8fNiWtGSzs8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute latency\n",
        "def measure_latency(model, input_tensor, repetitions=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        for _ in range(repetitions):\n",
        "            _ = model(input_tensor)\n",
        "        end = time.time()\n",
        "    latency = (end - start) / repetitions\n",
        "    return latency\n",
        "\n",
        "\n",
        "\n",
        "# Measure FLOPs and number of parameters\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 1024, 512).to(device)\n",
        "\n",
        "height = 1024\n",
        "width = 512\n",
        "image =torch.zeros((1,3, height, width)).to(device)   # torch.randn(1,3, 1024, 512).to(device)#\n",
        "\n",
        "model.eval()\n",
        "flops = FlopCountAnalysis(model, image)\n",
        "print(flop_count_table(flops))\n",
        "\n"
      ],
      "metadata": {
        "id": "e8p-LLxDzMNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure FLOPs and parameters using torchprofile\n",
        "dummy_input = torch.randn(1, 3, 1024,512).to(device)\n",
        "model.eval()\n",
        "flops = torchprofile.profile_macs(model, args=(dummy_input,))\n",
        "\n",
        "params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f' flops={flops}\\n params={params} ')\n"
      ],
      "metadata": {
        "id": "ljOeeMI_Qlp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Mounting for Saving and Loading model**"
      ],
      "metadata": {
        "id": "YzK3ao5A0j0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import isdir\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/Drive')\n",
        "get_ipython().system('/content/Drive/MyDrive/Checkpoints/BiSeNet/lr0.001b4')"
      ],
      "metadata": {
        "id": "u5JGDA3jrpX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions for Saving and Loading Model**"
      ],
      "metadata": {
        "id": "aMtKvnOEugHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save the model\n",
        "def save_checkpoint(epoch, model, optimizer, save_dir='/content/Drive/MyDrive/Checkpoints/BiSeNet/lr0.001b4'):\n",
        "    # Ensure the save directory exists\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Define the model filename with the epoch number\n",
        "    checkpoint_filename = f'checkpoint_epoch_{epoch}.pth'\n",
        "    checkpoint_path = os.path.join(save_dir, checkpoint_filename)\n",
        "\n",
        "    # Save the model and optimizer state dictionaries\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "    print(f'Model and optimizer saved to {checkpoint_path}')\n",
        "\n",
        "# Function to load the model\n",
        "def load_checkpoint(epoch, model, optimizer, save_dir='/content/Drive/MyDrive/Checkpoints/BiSeNet/lr0.001b4'):\n",
        "    checkpoint_filename = f'checkpoint_epoch_{epoch}.pth'\n",
        "    checkpoint_path = os.path.join(save_dir, checkpoint_filename)\n",
        "\n",
        "    # Check if the file exists\n",
        "    if not os.path.isfile(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"The specified file was not found: {checkpoint_path}\")\n",
        "\n",
        "    # Load the model and optimizer state dictionaries\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "    print(f'Model and optimizer loaded from {checkpoint_path}, resuming at epoch {start_epoch}')\n",
        "    return model, optimizer, start_epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "1hgP366Auk-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMpHUqlvulCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training with  Saving and Loading**"
      ],
      "metadata": {
        "id": "9Iwy_KA5TNwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#train loop\n",
        "# Resume training from the last checkpoint if available\n",
        "resume_training =False   # Set this to True if you want to resume training\n",
        "\n",
        "epochs = 50 # Set this to the total number of epochs you want to train\n",
        "\n",
        "if resume_training:\n",
        "    epoch_to_resume =0  #9,19,29,39,49  # Set this to the epoch from which you want to resume\n",
        "    try:\n",
        "        model, optimizer, start_epoch = load_checkpoint(epoch_to_resume, model, optimizer)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "        start_epoch = 0\n",
        "else:\n",
        "    start_epoch = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    miou_metric.reset()\n",
        "    counter = 1\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs[0], labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "       # print(counter)\n",
        "        counter += 1\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        miou_metric.update(outputs[0].argmax(dim=1), labels)\n",
        "\n",
        "    # Save the model every 10 epochs\n",
        "    if (epoch+1) % 2 == 0:\n",
        "        save_checkpoint(epoch, model, optimizer)\n",
        "\n",
        "    miou = miou_metric.compute().item()\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}, Train mIoU: {miou}')\n",
        "\n",
        "# Measure latency after training\n",
        "latency = measure_latency(model, dummy_input)\n",
        "print(f\"Latency: {latency:.6f} seconds\")\n",
        "\n",
        "print(\"Training completed!\")\n"
      ],
      "metadata": {
        "id": "P8iDHubdrIAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Loader**"
      ],
      "metadata": {
        "id": "OpQ67CRAKM-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform if you need to preprocess the images\n",
        "transformed_dataset = CityscapesDataset(root_dir='/content/Drive/MyDrive/Cityscapes/Cityspaces/gtFine/val',\n",
        "im_transform=transforms.Compose([\n",
        "\n",
        "transforms.ToTensor(),\n",
        "transforms.Resize(test_resolution),\n",
        "\n",
        "]), )\n",
        "\n",
        "\n",
        "test_loader = DataLoader(transformed_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8F7HxrMSK3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference and mIoU calculation on test set**"
      ],
      "metadata": {
        "id": "AZ3uZ37_S_5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "miou_metric.reset()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "\n",
        "\n",
        "       #labels = labels.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "        miou_metric.update(outputs.argmax(dim=1), labels)\n",
        "\n",
        "miou = miou_metric.compute().item()\n",
        "print(f'Test mIoU: {miou}')\n"
      ],
      "metadata": {
        "id": "KZd3_wXY4DbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkSs0iVQ_Vox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}